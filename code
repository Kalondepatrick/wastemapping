#############################################
# Setting up the environment
############################################

# Libraries
library(keras) 
library(tensorflow) 
library(tfdatasets) 
library(purrr) 
library(ggplot2)
library(rsample)
library(sf)
library(stars)
library(raster)
library(reticulate)
library(mapview)



##################################################
# Building the Model
#################################################

first_model <- keras_model_sequential()
layer_conv_2d(first_model, filters = 32, kernel_size = 3, activation = "relu", input_shape = c(128,128,3))
layer_max_pooling_2d(first_model, pool_size = c(2,2))
layer_conv_2d(first_model, filters = 64, kernel_size = c(3,3), activation = "relu") 
layer_max_pooling_2d(first_model, pool_size = c(2,2))
layer_conv_2d(first_model, filters = 128, kernel_size = c(3,3), activation = "relu")
layer_max_pooling_2d(first_model, pool_size = c(2,2)) 
layer_conv_2d(first_model, filters = 128, kernel_size = c(3,3), activation = "relu")
layer_max_pooling_2d(first_model, pool_size = c(2,2)) 
layer_flatten(first_model) 
layer_dense(first_model, units = 256, activation = "relu") 
layer_dense(first_model, units = 1, activation = "sigmoid")

#The model
summary(first_model)



##################################################
# Preparing the data
#################################################
#setwd("C:/Thesis/SpatialAnalysis/CNN/CodeAttempt")

#Set Working directory
setwd("~/Documents/Studies/MScThesis/MachineLearning/ML")


#Getting all file paths containing our targets
subset_list <- list.files("./training/true", full.names = T)

#Create a dataframe with two columns: file paths and labels (1)
data_true <- data.frame(img=subset_list, lbl=rep(1L, length(subset_list)))


#Getting all file paths containing non-targets
subset_list <- list.files("./training/false", full.names = T)

#Create a dataframe with two columns: file paths and labels (1)
data_false <- data.frame(img=subset_list, lbl=rep(0L, length(subset_list)))

#Merge the two dataframes
data <- rbind(data_true, data_false)

#Randonly split data into 75 percent training and 25 percent testing. The split should be done proportional of the two categories

set.seed(2020)
data <- initial_split(data, prop = 0.75, strata = "lbl")

## Looking at the data
data
head(training(data))
c(nrow(training(data)[training(data)$lbl==0,]), nrow(training(data)[training(data)$lbl==1,])) #Check equal split #That is 0's and 1's



training_dataset <- tensor_slices_dataset(training(data)) 

#tfe_enable_eager_execution()
## A List of all tensors
dataset_iterator <- as_iterator(training_dataset)
dataset_list <- iterate(dataset_iterator)
head(dataset_list)



######-------------------########

#get input shape expected by first_model
subset_size <- first_model$input_shape[2:3]

# apply function on each dataset element: function is list_modify()
#->modify list item "img" three times:

# 1 read decode jpeg
training_dataset <- 
  dataset_map(training_dataset, function(.x)
    list_modify(.x, img = tf$image$decode_jpeg(tf$io$read_file(.x$img))))

# 2 convert data type
training_dataset <- 
  dataset_map(training_dataset, function(.x)
    list_modify(.x, img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32)))

# 3 resize to the size expected by model
training_dataset <- 
  dataset_map(training_dataset, function(.x)
    list_modify(.x, img = tf$image$resize(.x$img, size = shape(subset_size[1], subset_size[2]))))


### ------------------###########


## The data has 0's then targets. Shuffle the data

training_dataset<-dataset_shuffle(training_dataset, buffer_size = 10L*128)
#Create batches for data processing
training_dataset<-dataset_batch(training_dataset, 10L)

training_dataset<-dataset_map(training_dataset, unname)



##########################################################
#   Looking at elements of the dataset                  ##
##########################################################
dataset_iterator <- as_iterator(training_dataset)
dataset_list <- iterate(dataset_iterator)
head(dataset_list)
dataset_list[[1]][[1]]



############################################################
#             VALIDATION                                  ##
############################################################


validation_dataset <- tensor_slices_dataset(testing(data)) 

# 1 read decode jpeg
validation_dataset <- 
  dataset_map(validation_dataset, function(.x)
    list_modify(.x, img = tf$image$decode_jpeg(tf$io$read_file(.x$img))))

# 2 convert data type
validation_dataset <- 
  dataset_map(validation_dataset, function(.x)
    list_modify(.x, img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32)))

# 3 resize to the size expected by model
validation_dataset <- 
  dataset_map(validation_dataset, function(.x)
    list_modify(.x, img = tf$image$resize(.x$img, size = shape(subset_size[1], subset_size[2]))))


#Create batches for data processing
validation_dataset<-dataset_batch(validation_dataset, 10L)

validation_dataset<-dataset_map(validation_dataset, unname)




############################################################
#             TRAINING                                    ##
############################################################


compile(
  first_model,
  optimizer = optimizer_rmsprop(lr = 5e-5),
  loss = "binary_crossentropy",
  metrics = "accuracy"
)


diagnostics <- fit(first_model,
                   training_dataset,
                   epochs = 100,
                   validation_data = validation_dataset)
plot(diagnostics)


#############################################
#    As you continue training, fitting new data sets makes validation data to look more than the data that was used for model development, and when the graph is flat it means that there is  no effect on validation = overfitting 



######  PREDICTION

predictions <- predict(first_model, validation_dataset)
head(predictions)
tail(predictions)





par(mfrow=c(1,3),mai=c(0.1,0.1,0.3,0.1),cex=0.8)
    for(i in 1:3){
      sample <- floor(runif(n = 1,min = 1,max = 56))
      img_path <- as.character(testing(data)[[sample,1]])
      img <- stack(img_path)
      plotRGB(img,margins=T,main = paste("prediction:",round(predictions[sample],digits=3)," | ","label:",as.character(testing(data)[[sample,2]])))
    }


### Error in col[[i, exact = exact]] : subscript out of bounds
